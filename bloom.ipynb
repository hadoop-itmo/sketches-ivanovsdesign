{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmh3\n",
    "import math\n",
    "from bitarray import bitarray\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random \n",
    "import string\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import gen_uniq_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1\n",
    "\n",
    "Bloom-filter на одной хэщ-функции\n",
    "C целыми числами скорость около 300 ит/сек ~18000 строк в минуту. Слишком долго ждать. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BloomFilterInt:\n",
    "    '''\n",
    "    Bloom-filter on Python integers\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 n: int):\n",
    "        self.n = n\n",
    "        self.bit_array = 0\n",
    "\n",
    "    def _hash(self, s):\n",
    "        return mmh3.hash(s, 0) % self.n\n",
    "\n",
    "    def put(self, s):\n",
    "        self.bit_array |= 1 << self._hash(s)\n",
    "\n",
    "    def get(self, s):\n",
    "        return self.bit_array & (1 << self._hash(s)) != 0\n",
    "\n",
    "    def size(self):\n",
    "        return bin(self.bit_array).count('1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем реализовать с помощью numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BloomFilterNumpy:\n",
    "    '''\n",
    "    Bloom-filter using numpy bit array\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 n: int):\n",
    "        self.n = n\n",
    "        self.bit_array = np.zeros(n, dtype=bool)  # Use a numpy array of booleans\n",
    "\n",
    "    def _hash(self, s):\n",
    "        # Generate a single hash value for the given input\n",
    "        return mmh3.hash(s, 0) % self.n\n",
    "\n",
    "    def put(self, s):\n",
    "        # Set the bit corresponding to the hash value\n",
    "        hash_value = self._hash(s)\n",
    "        self.bit_array[hash_value] = True\n",
    "\n",
    "    def get(self, s):\n",
    "        # Check if the bit corresponding to the hash value is set\n",
    "        hash_value = self._hash(s)\n",
    "        return self.bit_array[hash_value]\n",
    "\n",
    "    def size(self):\n",
    "        # Count the number of set bits in the bit array\n",
    "        return np.sum(self.bit_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf_sizes = [8, 64, 1024, 65536, 16777216]\n",
    "set_sizes = [5, 50, 500, 5000, 5000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n"
     ]
    }
   ],
   "source": [
    "for set_size in set_sizes:\n",
    "    gen_uniq_seq(name = str(set_size),\n",
    "                n_records = set_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использовал tqdm для контроля за процессом - избыточно, можно было обойтись просто выводом в stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    }
   ],
   "source": [
    "result_np = []\n",
    "\n",
    "for bf_size in tqdm(bf_sizes,\n",
    "                    total=len(set_sizes),\n",
    "                    position=0,\n",
    "                    leave=False,\n",
    "                    desc='Iterating through bf sizes'):\n",
    "    for set_size in tqdm(set_sizes,\n",
    "                            total=len(set_sizes),\n",
    "                            position=1,\n",
    "                            leave=False,\n",
    "                            desc='Iterating through set sizes'):\n",
    "\n",
    "        bf_int = BloomFilterNumpy(n = bf_size)\n",
    "\n",
    "        fp_count = 0\n",
    "\n",
    "        with open(f'{set_size}') as file: \n",
    "            for line in file:\n",
    "                if bf_int.get(line):\n",
    "                    fp_count += 1\n",
    "                bf_int.put(line)\n",
    "            ones_count_int = bf_int.size()\n",
    "\n",
    "        result_np.append(\n",
    "            {\n",
    "                'bf_size' : bf_size,\n",
    "                'set_size' : set_size,\n",
    "                'fp_count' : fp_count,\n",
    "                'ones_count' : ones_count_int\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bf_size</th>\n",
       "      <th>set_size</th>\n",
       "      <th>fp_count</th>\n",
       "      <th>ones_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>50</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>500</td>\n",
       "      <td>492</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>5000</td>\n",
       "      <td>4992</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>5000000</td>\n",
       "      <td>4999992</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>64</td>\n",
       "      <td>50</td>\n",
       "      <td>15</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>64</td>\n",
       "      <td>500</td>\n",
       "      <td>436</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>64</td>\n",
       "      <td>5000</td>\n",
       "      <td>4936</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>64</td>\n",
       "      <td>5000000</td>\n",
       "      <td>4999936</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1024</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1024</td>\n",
       "      <td>500</td>\n",
       "      <td>104</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1024</td>\n",
       "      <td>5000</td>\n",
       "      <td>3986</td>\n",
       "      <td>1014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1024</td>\n",
       "      <td>5000000</td>\n",
       "      <td>4998976</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>65536</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>65536</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>65536</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>65536</td>\n",
       "      <td>5000</td>\n",
       "      <td>174</td>\n",
       "      <td>4826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>65536</td>\n",
       "      <td>5000000</td>\n",
       "      <td>4934464</td>\n",
       "      <td>65536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>16777216</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>16777216</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>16777216</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>16777216</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>16777216</td>\n",
       "      <td>5000000</td>\n",
       "      <td>676830</td>\n",
       "      <td>4323170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     bf_size  set_size  fp_count  ones_count\n",
       "0          8         5         0           5\n",
       "1          8        50        42           8\n",
       "2          8       500       492           8\n",
       "3          8      5000      4992           8\n",
       "4          8   5000000   4999992           8\n",
       "5         64         5         0           5\n",
       "6         64        50        15          35\n",
       "7         64       500       436          64\n",
       "8         64      5000      4936          64\n",
       "9         64   5000000   4999936          64\n",
       "10      1024         5         0           5\n",
       "11      1024        50         1          49\n",
       "12      1024       500       104         396\n",
       "13      1024      5000      3986        1014\n",
       "14      1024   5000000   4998976        1024\n",
       "15     65536         5         0           5\n",
       "16     65536        50         0          50\n",
       "17     65536       500         0         500\n",
       "18     65536      5000       174        4826\n",
       "19     65536   5000000   4934464       65536\n",
       "20  16777216         5         0           5\n",
       "21  16777216        50         0          50\n",
       "22  16777216       500         0         500\n",
       "23  16777216      5000         0        5000\n",
       "24  16777216   5000000    676830     4323170"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(result_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что с увеличением `bf_size` уменьшается число фолс позитив и увеличивается ones count. Видим важность правильного подбора размера фильтра. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2\n",
    "\n",
    "Оптимальное число хэш-функций k для данных\n",
    "\n",
    "$$\n",
    "    k = \\ln(2) \\cdot m/n\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BloomFilterNumpy:\n",
    "    def __init__(self,\n",
    "                 n: int,\n",
    "                 k: int):\n",
    "        \"\"\"\n",
    "        Bloom filter with k hash funcs\n",
    "        implemented with numpy\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.k = k\n",
    "        self.bit_array = np.zeros(n, dtype=bool)\n",
    "\n",
    "    def _hashes(self, item):\n",
    "        return [mmh3.hash(item, seed) % self.n for seed in range(self.k)]\n",
    "\n",
    "    def put(self, item):\n",
    "        for hash_value in self._hashes(item):\n",
    "            self.bit_array[hash_value] = True\n",
    "\n",
    "    def get(self, item):\n",
    "        return all(self.bit_array[hash_value] for hash_value in self._hashes(item))\n",
    "\n",
    "    def size(self):\n",
    "        return np.sum(self.bit_array) / self.k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment with 1 hash functions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment with 2 hash functions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment with 3 hash functions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment with 4 hash functions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    }
   ],
   "source": [
    "result_np_k = []\n",
    "\n",
    "for k in [1, 2, 3, 4]:\n",
    "    print(f'Experiment with {k} hash functions')\n",
    "    for bf_size in tqdm(bf_sizes,\n",
    "                        total=len(set_sizes),\n",
    "                        position=0,\n",
    "                        leave=False,\n",
    "                        desc='Iterating through bf sizes'):\n",
    "        for set_size in tqdm(set_sizes,\n",
    "                                total=len(set_sizes),\n",
    "                                position=1,\n",
    "                                leave=False,\n",
    "                                desc='Iterating through set sizes'):\n",
    "\n",
    "            bf_int = BloomFilterNumpy(n = bf_size,\n",
    "                                      k = k)\n",
    "\n",
    "            fp_count = 0\n",
    "\n",
    "            with open(f'{set_size}') as file: \n",
    "                for line in file:\n",
    "                    if bf_int.get(line):\n",
    "                        fp_count += 1\n",
    "                    bf_int.put(line)\n",
    "                ones_count_int = bf_int.size()\n",
    "\n",
    "            result_np_k.append(\n",
    "                {\n",
    "                    'k' : k,\n",
    "                    'bf_size' : bf_size,\n",
    "                    'set_size' : set_size,\n",
    "                    'fp_count' : fp_count,\n",
    "                    'ones_count' : ones_count_int\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_df = pd.DataFrame(result_np_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что, например, с `k=2` fp_count снизился в разы для "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>k</th>\n",
       "      <th>bf_size</th>\n",
       "      <th>set_size</th>\n",
       "      <th>fp_count</th>\n",
       "      <th>ones_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>50</td>\n",
       "      <td>44</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>500</td>\n",
       "      <td>494</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>5000</td>\n",
       "      <td>4994</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>5000000</td>\n",
       "      <td>4999994</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>50</td>\n",
       "      <td>15</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>500</td>\n",
       "      <td>454</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>5000</td>\n",
       "      <td>4952</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>5000000</td>\n",
       "      <td>4999955</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>1024</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>48.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>1024</td>\n",
       "      <td>500</td>\n",
       "      <td>91</td>\n",
       "      <td>317.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>1024</td>\n",
       "      <td>5000</td>\n",
       "      <td>4221</td>\n",
       "      <td>512.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>2</td>\n",
       "      <td>1024</td>\n",
       "      <td>5000000</td>\n",
       "      <td>4999229</td>\n",
       "      <td>512.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>65536</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "      <td>65536</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>65536</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>496.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>65536</td>\n",
       "      <td>5000</td>\n",
       "      <td>37</td>\n",
       "      <td>4640.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>65536</td>\n",
       "      <td>5000000</td>\n",
       "      <td>4950895</td>\n",
       "      <td>32768.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>16777216</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>16777216</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "      <td>16777216</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>16777216</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>4998.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>2</td>\n",
       "      <td>16777216</td>\n",
       "      <td>5000000</td>\n",
       "      <td>388002</td>\n",
       "      <td>3765852.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  k   bf_size  set_size  fp_count  ones_count\n",
       "25          25  2         8         5         0         4.0\n",
       "26          26  2         8        50        44         4.0\n",
       "27          27  2         8       500       494         4.0\n",
       "28          28  2         8      5000      4994         4.0\n",
       "29          29  2         8   5000000   4999994         4.0\n",
       "30          30  2        64         5         0         5.0\n",
       "31          31  2        64        50        15        25.0\n",
       "32          32  2        64       500       454        32.0\n",
       "33          33  2        64      5000      4952        32.0\n",
       "34          34  2        64   5000000   4999955        32.0\n",
       "35          35  2      1024         5         0         5.0\n",
       "36          36  2      1024        50         0        48.5\n",
       "37          37  2      1024       500        91       317.5\n",
       "38          38  2      1024      5000      4221       512.0\n",
       "39          39  2      1024   5000000   4999229       512.0\n",
       "40          40  2     65536         5         0         5.0\n",
       "41          41  2     65536        50         0        50.0\n",
       "42          42  2     65536       500         0       496.5\n",
       "43          43  2     65536      5000        37      4640.0\n",
       "44          44  2     65536   5000000   4950895     32768.0\n",
       "45          45  2  16777216         5         0         5.0\n",
       "46          46  2  16777216        50         0        50.0\n",
       "47          47  2  16777216       500         0       500.0\n",
       "48          48  2  16777216      5000         0      4998.5\n",
       "49          49  2  16777216   5000000    388002   3765852.0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_df[k_df['k'] == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем общуюю статистику, чтобы увидеть, как изменяется число false positives и ones count. Для наглядности возьмем максимальный размер bf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>k</th>\n",
       "      <th>bf_size</th>\n",
       "      <th>set_size</th>\n",
       "      <th>fp_count</th>\n",
       "      <th>ones_count</th>\n",
       "      <th>fp_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>16777216</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>16777216</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>5.000000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>16777216</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>5.000000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>16777216</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>5.000000e+03</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>16777216</td>\n",
       "      <td>5000000</td>\n",
       "      <td>676830</td>\n",
       "      <td>4.323170e+06</td>\n",
       "      <td>0.135366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>16777216</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>16777216</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>5.000000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "      <td>16777216</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>5.000000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>16777216</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>4.998500e+03</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>2</td>\n",
       "      <td>16777216</td>\n",
       "      <td>5000000</td>\n",
       "      <td>388002</td>\n",
       "      <td>3.765852e+06</td>\n",
       "      <td>0.077600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>16777216</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>71</td>\n",
       "      <td>3</td>\n",
       "      <td>16777216</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>5.000000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>72</td>\n",
       "      <td>3</td>\n",
       "      <td>16777216</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>5.000000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>73</td>\n",
       "      <td>3</td>\n",
       "      <td>16777216</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>4.998000e+03</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>74</td>\n",
       "      <td>3</td>\n",
       "      <td>16777216</td>\n",
       "      <td>5000000</td>\n",
       "      <td>333943</td>\n",
       "      <td>3.304259e+06</td>\n",
       "      <td>0.066789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>4</td>\n",
       "      <td>16777216</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>4</td>\n",
       "      <td>16777216</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>5.000000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>4</td>\n",
       "      <td>16777216</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>5.000000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>4</td>\n",
       "      <td>16777216</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>4.996750e+03</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>4</td>\n",
       "      <td>16777216</td>\n",
       "      <td>5000000</td>\n",
       "      <td>343395</td>\n",
       "      <td>2.920514e+06</td>\n",
       "      <td>0.068679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  k   bf_size  set_size  fp_count    ones_count   fp_rate\n",
       "20          20  1  16777216         5         0  5.000000e+00  0.000000\n",
       "21          21  1  16777216        50         0  5.000000e+01  0.000000\n",
       "22          22  1  16777216       500         0  5.000000e+02  0.000000\n",
       "23          23  1  16777216      5000         0  5.000000e+03  0.000000\n",
       "24          24  1  16777216   5000000    676830  4.323170e+06  0.135366\n",
       "45          45  2  16777216         5         0  5.000000e+00  0.000000\n",
       "46          46  2  16777216        50         0  5.000000e+01  0.000000\n",
       "47          47  2  16777216       500         0  5.000000e+02  0.000000\n",
       "48          48  2  16777216      5000         0  4.998500e+03  0.000000\n",
       "49          49  2  16777216   5000000    388002  3.765852e+06  0.077600\n",
       "70          70  3  16777216         5         0  5.000000e+00  0.000000\n",
       "71          71  3  16777216        50         0  5.000000e+01  0.000000\n",
       "72          72  3  16777216       500         0  5.000000e+02  0.000000\n",
       "73          73  3  16777216      5000         0  4.998000e+03  0.000000\n",
       "74          74  3  16777216   5000000    333943  3.304259e+06  0.066789\n",
       "95          95  4  16777216         5         0  5.000000e+00  0.000000\n",
       "96          96  4  16777216        50         0  5.000000e+01  0.000000\n",
       "97          97  4  16777216       500         0  5.000000e+02  0.000000\n",
       "98          98  4  16777216      5000         0  4.996750e+03  0.000000\n",
       "99          99  4  16777216   5000000    343395  2.920514e+06  0.068679"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate false positive rate\n",
    "k_df['fp_rate'] = k_df['fp_count'] / k_df['set_size']\n",
    "\n",
    "k_df[k_df['bf_size'] == 16777216]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что с увеличением числа хэш-функций до 3 включительно число fp уменьшается, однако при 4 хэш-функциях оно больше, чем при 3. Слишком большое количество хэш-функций может привести к увеличению числа единиц в фильтре (битовые позиции становятся единицами), что увеличивает шансы на ложные срабатывания. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем Counting Bloom Filter на `k` хэш-функций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CountingBloomFilter:\n",
    "    def __init__(self,\n",
    "                 k: int,\n",
    "                 n: int,\n",
    "                 cap: int):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        k (int): Number of hash functions.\n",
    "        n (int): Number of counters.\n",
    "        cap (int): Number of bits per counter.\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.n = n\n",
    "        self.cap = cap\n",
    "\n",
    "        # Calculate the number of counters that fit into a 64-bit integer\n",
    "        counters_per_int = 64 // cap\n",
    "        self.counters_per_int = counters_per_int\n",
    "\n",
    "        # Calculate how many 64-bit integers we need\n",
    "        num_ints = (n + counters_per_int - 1) // counters_per_int\n",
    "        self.num_ints = num_ints\n",
    "\n",
    "        # Initialize the bit array\n",
    "        self.bit_array = np.zeros(num_ints, dtype=np.uint64)\n",
    "\n",
    "    def _hashes(self, item):\n",
    "        \"\"\"Generate k hash values for the item using different seeds.\"\"\"\n",
    "        return [mmh3.hash(item, seed) % self.n for seed in range(self.k)]\n",
    "\n",
    "    def _get_counter_index_and_offset(self, hash_value):\n",
    "        \"\"\"Calculate the index and bit offset for the given hash value.\"\"\"\n",
    "        int_index = hash_value // self.counters_per_int\n",
    "        bit_offset = (hash_value % self.counters_per_int) * self.cap\n",
    "        return int_index, bit_offset\n",
    "\n",
    "    def put(self, item):\n",
    "        \"\"\"Insert an item into the Counting Bloom Filter.\"\"\"\n",
    "        for hash_value in self._hashes(item):\n",
    "            int_index, bit_offset = self._get_counter_index_and_offset(hash_value)\n",
    "            # Extract the counter\n",
    "            mask = (1 << self.cap) - 1\n",
    "            current_count = (self.bit_array[int_index] >> bit_offset) & mask\n",
    "            # Increment the counter if it is not at its maximum\n",
    "            if current_count < mask:\n",
    "                self.bit_array[int_index] += (1 << bit_offset)\n",
    "\n",
    "    def get(self, item):\n",
    "        \"\"\"Check if an item is in the Counting Bloom Filter.\"\"\"\n",
    "        for hash_value in self._hashes(item):\n",
    "            int_index, bit_offset = self._get_counter_index_and_offset(hash_value)\n",
    "            # Extract the counter\n",
    "            mask = (1 << self.cap) - 1\n",
    "            current_count = (self.bit_array[int_index] >> bit_offset) & mask\n",
    "            if current_count == 0:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def size(self):\n",
    "        \"\"\"Return the sum of all counters divided by k.\"\"\"\n",
    "        mask = (1 << self.cap) - 1\n",
    "        total_count = 0\n",
    "        for int_index in range(self.num_ints):\n",
    "            value = self.bit_array[int_index]\n",
    "            for _ in range(self.counters_per_int):\n",
    "                total_count += value & mask\n",
    "                value >>= self.cap\n",
    "        return total_count / self.k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Низкий cap может привести к переполнению счетчиков, в то время как слишком высокий cap увеличивает использование памяти без необходимости.\n",
    "\n",
    "Для разных задач оптимальные разные комбинации `cap`, `k`, `bf_size`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Комбинации для экспериментов с Counting Bloom Filter\n",
    "\n",
    "#### 1. Сценарий с высоким потоком данных\n",
    "- **Параметры**: `cap = 2`, `k = 3`, `sample_size = 10,000`, `bf_size = 65,536 бит` (ближайшая степень двойки)\n",
    "- **Описание**: Подходит для сценариев с высокой нагрузкой и частым добавлением/удалением элементов. Размер фильтра поддерживает низкую вероятность ложных срабатываний при большом ожидаемом количестве элементов.\n",
    "\n",
    "#### 2. Сценарий с редкими изменениями\n",
    "- **Параметры**: `cap = 5`, `k = 2`, `sample_size = 1,000`, `bf_size = 8,192 бит` (степень двойки)\n",
    "- **Описание**: Для приложений, где элементы редко удаляются. Более высокий `cap` позволяет больше добавлений без переполнения, а меньший размер фильтра подходит для меньшего количества элементов.\n",
    "\n",
    "#### 3. Баланс между точностью и использованием памяти\n",
    "- **Параметры**: `cap = 3`, `k = 4`, `sample_size = 5,000`, `bf_size = 32,768 бит` (степень двойки)\n",
    "- **Описание**: Баланс между использованием памяти и вероятностью ложных срабатываний, подходящий для средних по размеру наборов данных с необходимостью умеренной точности.\n",
    "\n",
    "#### 4. Сценарий с высокой точностью\n",
    "- **Параметры**: `cap = 4`, `k = 5`, `sample_size = 2,000`, `bf_size = 32,768 бит` (степень двойки)\n",
    "- **Описание**: Максимизирует точность с большим количеством хэш-функций и умеренным `cap`, подходит для приложений, где важно минимизировать ложные срабатывания.\n",
    "\n",
    "#### 5. Эксперимент с количеством битов счетчика, не делящимся на 64\n",
    "- **Параметры**: `cap = 3`, `k = 3`, `sample_size = 3,000`, `bf_size = 16,384 бит` (степень двойки), **битов на счетчик**: 5\n",
    "- **Описание**: Проверка нестандартной битности\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_experiment(cap: int,\n",
    "                   k: int,\n",
    "                   bf_size: int,\n",
    "                   set_size: int) -> tuple[int, float]:\n",
    "    \"\"\"\n",
    "    Runs experiment with cap bloom filter\n",
    "    \"\"\"\n",
    "    gen_uniq_seq(f'cap_{set_size}.csv', 5000)\n",
    "    counting_bloom_filter = CountingBloomFilter(cap=cap,\n",
    "                                                k=k,\n",
    "                                                n=bf_size)\n",
    "    fp_count = 0\n",
    "\n",
    "    with open(f'cap_{set_size}.csv') as file: \n",
    "        for line in file:\n",
    "            if counting_bloom_filter.get(line):\n",
    "                fp_count += 1\n",
    "            counting_bloom_filter.put(line)\n",
    "        ones_count = counting_bloom_filter.size()\n",
    "\n",
    "    return fp_count, ones_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Experiment: cap=2, k=3, bf_size=65536, set_size=10000\n",
      "  False Positives: 6, Ones in Filter: 4999.0\n",
      "Experiment: cap=5, k=2, bf_size=8192, set_size=1000\n",
      "  False Positives: 1109, Ones in Filter: 5000.0\n",
      "Experiment: cap=3, k=4, bf_size=32768, set_size=5000\n",
      "  False Positives: 58, Ones in Filter: 5000.0\n",
      "Experiment: cap=4, k=5, bf_size=32768, set_size=2000\n",
      "  False Positives: 55, Ones in Filter: 5000.0\n",
      "Experiment: cap=3, k=3, bf_size=16384, set_size=3000\n",
      "  False Positives: 338, Ones in Filter: 5000.0\n"
     ]
    }
   ],
   "source": [
    "def run_cap_experiments():\n",
    "    \"\"\"\n",
    "    Defines experiments with counting Bloom Filter\n",
    "    \"\"\"\n",
    "    experiments = [\n",
    "        {\"cap\": 2, \"k\": 3, \"bf_size\": 65536, \"set_size\": 10000},\n",
    "        {\"cap\": 5, \"k\": 2, \"bf_size\": 8192, \"set_size\": 1000},\n",
    "        {\"cap\": 3, \"k\": 4, \"bf_size\": 32768, \"set_size\": 5000},\n",
    "        {\"cap\": 4, \"k\": 5, \"bf_size\": 32768, \"set_size\": 2000},\n",
    "        {\"cap\": 3, \"k\": 3, \"bf_size\": 16384, \"set_size\": 3000}  # 5-bit counter not directly specified in parameters\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for exp in experiments:\n",
    "        fp_count, ones_count = cap_experiment(\n",
    "            cap=exp[\"cap\"],\n",
    "            k=exp[\"k\"],\n",
    "            bf_size=exp[\"bf_size\"],\n",
    "            set_size=exp[\"set_size\"]\n",
    "        )\n",
    "        results.append({\n",
    "            \"cap\": exp[\"cap\"],\n",
    "            \"k\": exp[\"k\"],\n",
    "            \"bf_size\": exp[\"bf_size\"],\n",
    "            \"set_size\": exp[\"set_size\"],\n",
    "            \"fp_count\": fp_count,\n",
    "            \"ones_count\": ones_count\n",
    "        })\n",
    "\n",
    "    for result in results:\n",
    "        print(f\"Experiment: cap={result['cap']}, k={result['k']}, bf_size={result['bf_size']}, set_size={result['set_size']}\")\n",
    "        print(f\"  False Positives: {result['fp_count']}, Ones in Filter: {result['ones_count']}\")\n",
    "\n",
    "run_cap_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Большой размер фильтра (эксперимнет 1) значительно снижает количество ложных срабатываний, даже при низкой емкости счетчиков.\n",
    "\n",
    "2. Маленький размер фильтра и высокое значение емкости счетчиков (эксперимент 2) приводят к высокому количеству ложных срабатываний из-за насыщения фильтра.\n",
    "\n",
    "3. Сбалансированный размер фильтра и количество хэш-функций (эксперимент 3 и 4) обеспечивают умеренное количество ложных срабатываний, но увеличение числа хэш-функций выше определенного уровня не дает значительных улучшений.\n",
    "\n",
    "4. Недостаточный размер фильтра (эксперимент 5) при умеренном количестве хэш-функций и емкости счетчиков увеличивает вероятность ложных срабатываний."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 4\n",
    "\n",
    "Реализация HyperLogLog\n",
    "\n",
    "Кажется, что аргументы инициализации k, n избыточны - ведь в общем случае hyperloglog принимает параметр точности b, который определяет число регистров. \n",
    "\n",
    "```\n",
    "m, where m = 2^b\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperLogLog:\n",
    "    def __init__(self, b: int):\n",
    "        self.b = b\n",
    "        self.m = 1 << b  # m = 2^b\n",
    "        self.registers = [0] * self.m\n",
    "        self.alpha_m = self.get_alpha_m(self.m)\n",
    "\n",
    "    def get_alpha_m(self, m):\n",
    "        # Constants for different m values\n",
    "        if m == 16:\n",
    "            return 0.673\n",
    "        elif m == 32:\n",
    "            return 0.697\n",
    "        elif m == 64:\n",
    "            return 0.709\n",
    "        else:\n",
    "            return 0.7213 / (1 + 1.079 / m)\n",
    "\n",
    "    def hash(self, value):\n",
    "        # Hash the value using MurmurHash and convert to a binary string\n",
    "        return mmh3.hash(value, signed=False)\n",
    "\n",
    "    def rho(self, w):\n",
    "        # Position of the leftmost 1-bit in the binary representation of w\n",
    "        # Add 1 because we want the position starting from 1, not 0\n",
    "        return len(w) - len(w.lstrip('0')) + 1\n",
    "\n",
    "    def put(self, item):\n",
    "        x = self.hash(item)\n",
    "        # Convert the hash to binary and ensure it has enough bits\n",
    "        x_bin = bin(x)[2:].zfill(32)  \n",
    "\n",
    "        # Use the first b bits for the register index\n",
    "        j = int(x_bin[:self.b], 2)\n",
    "        \n",
    "        # Use the remaining bits to calculate the rank (rho)\n",
    "        w = x_bin[self.b:]\n",
    "        self.registers[j] = max(self.registers[j], self.rho(w))\n",
    "\n",
    "    def est_size(self):\n",
    "        # Calculate the harmonic mean of 2^-M[j]\n",
    "        Z = sum(2.0 ** -reg for reg in self.registers)\n",
    "        E = self.alpha_m * self.m * self.m / Z\n",
    "\n",
    "        # Apply small range correction\n",
    "        if E <= 2.5 * self.m:\n",
    "            V = self.registers.count(0)\n",
    "            if V > 0:\n",
    "                E = self.m * math.log(self.m / V)\n",
    "\n",
    "        # Apply large range correction\n",
    "        elif E > (1 / 30.0) * (1 << 32):\n",
    "            E = -(1 << 32) * math.log(1 - E / (1 << 32))\n",
    "\n",
    "        return E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Планируем эксперименты с генерацией повторяющихся значений с использованием предоставленной функции `gen_grouped_seq`, также рассчитываем относительную ошибку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: grouped_seq_500.txt\n",
      "True size: 510\n",
      "Estimated size: 510.8829835851813\n",
      "Relative error: 0.1731%\n",
      "\n",
      "Dataset: grouped_seq_50000.txt\n",
      "True size: 40100\n",
      "Estimated size: 41545.498112794034\n",
      "Relative error: 3.6047%\n",
      "\n",
      "Dataset: grouped_seq_5000000.txt\n",
      "True size: 4001000\n",
      "Estimated size: 3995660.9960047817\n",
      "Relative error: 0.1334%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import gen_grouped_seq\n",
    "from typing import List\n",
    "\n",
    "def run_experiment(pattern: List,\n",
    "                   filename: str,\n",
    "                   true_size: int,\n",
    "                   b: int):\n",
    "    # Generate the dataset\n",
    "    gen_grouped_seq(filename, pattern)\n",
    "\n",
    "    # Initialize HyperLogLog\n",
    "    hll = HyperLogLog(b=b)\n",
    "\n",
    "    # Read the dataset and insert keys into HyperLogLog\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            key = line.strip().split(':')[0]\n",
    "            hll.put(key)\n",
    "\n",
    "    # Estimate the size\n",
    "    estimated_size = hll.est_size()\n",
    "    \n",
    "    # Calculate relative error\n",
    "    relative_error = abs(estimated_size - true_size) / true_size\n",
    "\n",
    "    print(f\"Dataset: {filename}\")\n",
    "    print(f\"True size: {true_size}\")\n",
    "    print(f\"Estimated size: {estimated_size}\")\n",
    "    print(f\"Relative error: {relative_error:.4%}\\n\")\n",
    "\n",
    "# Run experiments with different sizes and patterns\n",
    "run_experiment(pattern=[(500, 1), (10, 100)], filename=\"grouped_seq_500.txt\", true_size=510, b=14)\n",
    "run_experiment(pattern=[(40000, 1), (100, 100)], filename=\"grouped_seq_50000.txt\", true_size=40100, b=14)\n",
    "run_experiment(pattern=[(4000000, 1), (1000, 1000)], filename=\"grouped_seq_5000000.txt\", true_size=4001000, b=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можем видеть, относительная ошибка невысока. Для размера даннных 50 тыс. ошибка выше. Попробуем увеличить параметр точности с 14 до 18 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: grouped_seq_50000.txt\n",
      "True size: 40100\n",
      "Estimated size: 40086.54171819347\n",
      "Relative error: 0.0336%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_experiment(pattern=[(40000, 1), (100, 100)], filename=\"grouped_seq_50000.txt\", true_size=40100, b=18)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ошибка снизилась"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 5\n",
    "\n",
    "Васе подарили два csv-файла. Каждый из них содержит по 10 миллиардов строк.\n",
    "\n",
    "Вася хочет сделать JOIN по их первым колонкам, но опасается, что по некоторым ключам будет слишком много записей и JOIN будет работать слишком долго.\n",
    "\n",
    "Вася разбирается в том, как делать JOIN и он даже прибросил, что пороговым значением будет 60000. То есть если у двух таблиц будет общий ключ и в каждой таблице по этому ключу будет более 60000 записей - то будут проблемы.\n",
    "\n",
    "Вася попробовал посчитать наивным скриптом через Counter, но не хватило памяти.\n",
    "\n",
    "Напишите Васе скрипт. который посчитает и памяти которому хватит. Прочитать файлы несколько раз можно, но чем меньше - тем лучше. Два прохода по каждому файлу должно точно хватать. Но в некоторых случаях может хватить и меньшего. И этими случаями лучше воспользоваться.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сгенерируем файлы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Файлы с 10 млрд строк будут занимать очень много места ~1 TB, если генерировать с помощью предоставленных функций. \n",
    "\n",
    "Немного изменим генерацию ключей (уберем генерацию uuid для ключа), чтобы получать одинаковые ключи в обоих файлах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import random\n",
    "\n",
    "# Modify the key generation to ensure overlap\n",
    "def gen_grouped_seq_fixed_keys(name: str,\n",
    "                               pattern: List,\n",
    "                               *,\n",
    "                               n_extra_cols: int = 0,\n",
    "                               to_shuffle: bool = False):\n",
    "    '''\n",
    "    Generates keys without uuid\n",
    "    '''\n",
    "    def gen():\n",
    "        num = 0\n",
    "        for n_keys, n_records in pattern:\n",
    "            for i1 in range(n_keys):\n",
    "                # Fixed key for ensuring overlap\n",
    "                body = f\"key{i1 + num}\"\n",
    "                for i2 in range(n_records):\n",
    "                    for j in range(n_extra_cols):\n",
    "                        body += f\",{uuid.uuid4()}\"\n",
    "                    yield body\n",
    "            num += n_keys\n",
    "\n",
    "    if to_shuffle:\n",
    "        data = list(gen())\n",
    "        random.shuffle(data)\n",
    "        result = data\n",
    "    else:\n",
    "        result = gen()\n",
    "\n",
    "    with open(name, \"wt\") as f:\n",
    "        for v in result:\n",
    "            print(v, file=f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерируем файлы с паттерном, чтобы первые 10 ключей превышали заданный порог в `60 000`. Записей с такими ключами будет `700 000`, остальные 50 ключей будут иметь по `30 000` записей на каждый"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import gen_grouped_seq\n",
    "import os\n",
    "\n",
    "pattern = [(10, 70000), (50, 30000)]  # 10 keys with 70,000 records each, 50 keys with 30,000 records\n",
    "gen_grouped_seq_fixed_keys(\"file1.csv\", pattern, to_shuffle=True)\n",
    "gen_grouped_seq_fixed_keys(\"file2.csv\", pattern, to_shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем подход с контролем памяти - контролем размера `chunk_size` словаря. Когда словарь `key_counts` превышает установленный размер `chunk_size`, значения счетчиков записываются во временный файл. \n",
    "\n",
    "Читаем файл в два прохода: в первом считаем ключи и сохраняем во временные файлы, во втором - аггрегируем значения из временных файлов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys exceeding 60000 occurrences in both files: {'key2', 'key7', 'key8', 'key3', 'key5', 'key6', 'key1', 'key9', 'key4', 'key0'}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "def count_keys(file_path: str,\n",
    "               threshold: int) -> dict:\n",
    "    \"\"\"\n",
    "    Counts keys and saves it into temp files,\n",
    "    if memory threshold `chunk_size` was exceeded\n",
    "    \"\"\"\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    chunk_size = 60000  # Memory limit\n",
    "    key_counts = defaultdict(int)\n",
    "    temp_files = []\n",
    "\n",
    "    with open(file_path, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            key = row[0]\n",
    "            key_counts[key] += 1\n",
    "            \n",
    "            if len(key_counts) >= chunk_size:\n",
    "                temp_file = os.path.join(temp_dir, f'temp_{len(temp_files)}.csv')\n",
    "                with open(temp_file, 'w', newline='') as tempf:\n",
    "                    writer = csv.writer(tempf)\n",
    "                    for k, count in key_counts.items():\n",
    "                        writer.writerow([k, count])\n",
    "                temp_files.append(temp_file)\n",
    "                key_counts.clear()\n",
    "    \n",
    "    if key_counts:\n",
    "        temp_file = os.path.join(temp_dir, f'temp_{len(temp_files)}.csv')\n",
    "        with open(temp_file, 'w', newline='') as tempf:\n",
    "            writer = csv.writer(tempf)\n",
    "            for k, count in key_counts.items():\n",
    "                writer.writerow([k, count])\n",
    "        temp_files.append(temp_file)\n",
    "    \n",
    "    # Aggregating values from temp files\n",
    "    final_counts = defaultdict(int)\n",
    "    for temp_file in temp_files:\n",
    "        with open(temp_file, newline='') as tempf:\n",
    "            reader = csv.reader(tempf)\n",
    "            for row in reader:\n",
    "                key, count = row\n",
    "                final_counts[key] += int(count)\n",
    "        os.remove(temp_file)\n",
    "    \n",
    "    os.rmdir(temp_dir)\n",
    "\n",
    "    # Return keys which exceed the threshold\n",
    "    return {k for k, count in final_counts.items() if count > threshold}\n",
    "\n",
    "def find_common_heavy_keys(file1, file2, threshold):\n",
    "    # Get 'exceeding' keys from each file\n",
    "    heavy_keys_file1 = count_keys(file1, threshold)\n",
    "    heavy_keys_file2 = count_keys(file2, threshold)\n",
    "    \n",
    "    # Find intersection - common keys\n",
    "    common_heavy_keys = heavy_keys_file1.intersection(heavy_keys_file2)\n",
    "    \n",
    "    return common_heavy_keys\n",
    "\n",
    "\n",
    "file1 = 'file1.csv'\n",
    "file2 = 'file2.csv'\n",
    "threshold = 60000\n",
    "\n",
    "common_keys = find_common_heavy_keys(file1, file2, threshold)\n",
    "print(f\"Keys exceeding {threshold} occurrences in both files: {common_keys}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ожидаемо десять ключей, которые в файлах идут первыми, превышают установленный порог. Что мы и наблюдаем в выводе программы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 6 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для решения этой задачи мы можем использовать комбинацию Counting Bloom Filter и HyperLogLog. Эти структуры данных помогут нам оценить размер потенциального JOIN-а между двумя CSV-файлами, эффективно управляя памятью.\n",
    "\n",
    "### Объяснение решения:\n",
    "**Counting Bloom Filter**:\n",
    "\n",
    "Используется для проверки наличия ключей из одного файла в другом.\n",
    "В отличие от стандартного Bloom Filter, Counting Bloom Filter позволяет не только проверять наличие, но и учитывать количество вхождений, что полезно для нашего случая.\n",
    "Он поможет с высокой вероятностью определить, пересекаются ли ключи из двух файлов.\n",
    "\n",
    "**HyperLogLog**:\n",
    "\n",
    "Используется для оценки количества уникальных ключей в каждом файле.\n",
    "Это дает нам приблизительное представление о количестве уникальных ключей, что полезно для понимания масштабов пересечения.\n",
    "\n",
    "\n",
    "Алгоритм: \n",
    "\n",
    "- Первый проход по файлам:\n",
    "\n",
    "Создаем HyperLogLog для каждого файла, чтобы оценить количество уникальных ключей.\n",
    "Создаем Counting Bloom Filter для отслеживания встреченных ключей.\n",
    "\n",
    "- Второй проход (при необходимости):\n",
    "\n",
    "Если количество уникальных ключей в обоих файлах не превышает 1 миллион, мы можем точно подсчитать пересечение, используя обычные множества.\n",
    "Если количество уникальных ключей велико, и мы не можем точно подсчитать пересечение, используем Counting Bloom Filter для оценки количества общих ключей.\n",
    "\n",
    "**Вывод результатов**:\n",
    "\n",
    "Если количество общих ключей превышает 10 миллионов, мы можем просто сообщить, что размер JOIN-а велик.\n",
    "В других случаях даем разумно точную оценку на основе данных из Counting Bloom Filter и HyperLogLog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [!NOTE] Для тестирования решения константы из задания были снижены до `100 000` (с 10 млн)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подбираем следующие параметры для тестового прогона: \n",
    "\n",
    "```\n",
    "HyperLogLog(b=14)\n",
    "CountingBloomFilter(k=4, n=1000000, cap=4)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def process_file(file_path: str,\n",
    "                 bloom_filter: Callable,\n",
    "                 hll: Callable) -> None:\n",
    "    with open(file_path, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            key = row[0].strip()\n",
    "            hll.put(key)\n",
    "            bloom_filter.put(key)\n",
    "\n",
    "def estimate_join_size(file1, file2):\n",
    "    # Initialize HyperLogLog for estimating unique keys\n",
    "    hll1 = HyperLogLog(b=14)\n",
    "    hll2 = HyperLogLog(b=14)\n",
    "    \n",
    "    # Initialize Counting Bloom Filters\n",
    "    bloom_filter1 = CountingBloomFilter(k=4, n=1000000, cap=4)\n",
    "    bloom_filter2 = CountingBloomFilter(k=4, n=1000000, cap=4)\n",
    "    \n",
    "    # Process both files\n",
    "    process_file(file1, bloom_filter1, hll1)\n",
    "    process_file(file2, bloom_filter2, hll2)\n",
    "    \n",
    "    # Estimate unique counts\n",
    "    unique_count1 = hll1.est_size()\n",
    "    unique_count2 = hll2.est_size()\n",
    "    \n",
    "    # If both have less than 1 million unique keys, calculate exact intersection\n",
    "    if unique_count1 <= 10_000 and unique_count2 <= 10_000:\n",
    "        keys1 = set()\n",
    "        keys2 = set()\n",
    "        with open(file1, newline='') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for row in reader:\n",
    "                keys1.add(row[0].strip())\n",
    "        \n",
    "        with open(file2, newline='') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for row in reader:\n",
    "                keys2.add(row[0].strip())\n",
    "        \n",
    "        exact_intersection = keys1.intersection(keys2)\n",
    "        print(f\"Exact intersection: {len(exact_intersection)}\")\n",
    "        return len(exact_intersection)\n",
    "    \n",
    "    # Estimate intersection size using Bloom Filters\n",
    "    intersection_estimate = 0\n",
    "    with open(file1, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            key = row[0].strip()\n",
    "            if bloom_filter2.get(key):\n",
    "                intersection_estimate += 1\n",
    "\n",
    "    # Determine if the estimated intersection size suggests a large join\n",
    "    if intersection_estimate > 100_000:\n",
    "        return \"> 100,000 (large join)\"\n",
    "    \n",
    "    print(f\"Estimated join size: {intersection_estimate}\")\n",
    "    return f\"Estimated join size: {intersection_estimate}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_shared_keys(file1_path: str,\n",
    "                    file2_path: str,\n",
    "                    shared_keys: List,\n",
    "                    unique_keys1: int,\n",
    "                    unique_keys2: int) -> None:\n",
    "    \"\"\"Generate two files with shared keys and unique keys.\"\"\"\n",
    "    with open(file1_path, \"w\") as f1, open(file2_path, \"w\") as f2:\n",
    "        for key in shared_keys:\n",
    "            f1.write(f\"{key}\\n\")\n",
    "            f2.write(f\"{key}\\n\")\n",
    "\n",
    "        for _ in range(unique_keys1):\n",
    "            f1.write(f\"{uuid.uuid4()}\\n\")\n",
    "\n",
    "        for _ in range(unique_keys2):\n",
    "            f2.write(f\"{uuid.uuid4()}\\n\")\n",
    "\n",
    "def run_experiments():\n",
    "    # Experiment 1: Files with less than 1 million unique keys for exact intersection\n",
    "    gen_uniq_seq(\"file1_exact.csv\", 5000)\n",
    "    gen_uniq_seq(\"file2_exact.csv\", 4500)\n",
    "    print(\"Experiment 1: Exact Intersection Test\")\n",
    "    print(estimate_join_size(\"file1_exact.csv\", \"file2_exact.csv\"))\n",
    "\n",
    "    # Experiment 2: Non-intersecting sets with high confidence of zero intersection\n",
    "    gen_uniq_seq(\"file1_non_intersect.csv\", 100000)\n",
    "    gen_uniq_seq(\"file2_non_intersect.csv\", 101000)\n",
    "    print(\"Experiment 2: Non-Intersecting Sets Test\")\n",
    "    print(estimate_join_size(\"file1_non_intersect.csv\", \"file2_non_intersect.csv\"))\n",
    "\n",
    "    # Experiment 3: Large join size detection (threshold 100,000)\n",
    "    shared_keys_large = [str(uuid.uuid4()) for _ in range(105_000)]\n",
    "    gen_shared_keys(\"file1_large_join.csv\", \"file2_large_join.csv\", shared_keys_large, 50_000, 50_000)\n",
    "    print(\"Experiment 3: Large Join Detection Test\")\n",
    "    print(estimate_join_size(\"file1_large_join.csv\", \"file2_large_join.csv\"))\n",
    "\n",
    "    # Experiment 4: Reasonably accurate estimation for moderate intersection\n",
    "    shared_keys_moderate = [str(uuid.uuid4()) for _ in range(40_000)]\n",
    "    gen_shared_keys(\"file1_moderate.csv\", \"file2_moderate.csv\", shared_keys_moderate, 40_000, 40_000)\n",
    "    print(\"Experiment 4: Moderate Intersection Estimation Test\")\n",
    "    print(estimate_join_size(\"file1_moderate.csv\", \"file2_moderate.csv\"))\n",
    "\n",
    "    # Clean up generated files\n",
    "    os.remove(\"file1_exact.csv\")\n",
    "    os.remove(\"file2_exact.csv\")\n",
    "    os.remove(\"file1_non_intersect.csv\")\n",
    "    os.remove(\"file2_non_intersect.csv\")\n",
    "    os.remove(\"file1_large_join.csv\")\n",
    "    os.remove(\"file2_large_join.csv\")\n",
    "    os.remove(\"file1_moderate.csv\")\n",
    "    os.remove(\"file2_moderate.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "Experiment 1: Exact Intersection Test\n",
      "Exact intersection: 0\n",
      "0\n",
      "0\n",
      "0\n",
      "Experiment 2: Non-Intersecting Sets Test\n",
      "Estimated join size: 1208\n",
      "Estimated join size: 1208\n",
      "Experiment 3: Large Join Detection Test\n",
      "> 100,000 (large join)\n",
      "Experiment 4: Moderate Intersection Estimation Test\n",
      "Estimated join size: 40242\n",
      "Estimated join size: 40242\n"
     ]
    }
   ],
   "source": [
    "run_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы видим, ошибка <2%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
